{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0727afcb-c3bc-4af9-a3b1-05a5c29fbc9b",
      "metadata": {},
      "source": [
        "# Generative AI Temelleri \n",
        "\n",
        "## 1. GenAI Temelleri ve Kavramlar\n",
        "\n",
        "* **LLM:** İstemleri olasılıksal olarak sürdüren, milyarlarca parametreli ve geniş metinlerle eğitilmiş genel amaçlı dil modelidir.\n",
        "* **Transformer Mimarisi:** Girdideki öğeler arası bağıntıları self-attention ile modelleyen, paralel hesaplamaya elverişli derin sinir ağı mimarisidir.\n",
        "* **Token:** Modelin işlediği en küçük birimdir; kelime, alt kelime veya karakter olabilir ve uzunluk/maliyet bu birimlerle ölçülür.\n",
        "* **Context Window:** Modelin tek seferde işleyip “hatırlayabildiği” azami token bütçesidir (girdi+çıktı) ve aşıldığında önceki bağlam dışarıda kalır.\n",
        "* **System Instructions:** Modelin rolünü, sınırlarını ve üslubunu belirleyen, diğer iletilere göre önceliği yüksek üst seviye talimatlardır.\n",
        "* **Hallucination:** Modelin gerçek veriye dayanmayan fakat tutarlı görünen bilgi uydurmasıdır ve RAG/ek doğrulama ile azaltılabilir.\n",
        "* **Thinking (Düşünme):** Modelin cevap vermeden önce bir iç muhakeme süreci yürütmesidir. Gemini 3 ve 2.5 serisi modellerde varsayılan olarak açıktır. `thinking_budget` parametresi ile kontrol edilir: 0 = kapalı, -1 = otomatik.\n",
        "* **Multimodal:** Tek bir modelin metin, görsel, ses ve video gibi farklı veri tiplerini aynı anda anlayıp işleyebilmesidir.\n",
        "\n",
        "## 2. LLM-Based Uygulama Geliştirme: Temel Enstrümanlar\n",
        "\n",
        "### 2.1. Model Seçimi \n",
        "\n",
        "- **API based modeller:** Claude, OpenAI, Google (Gemini) vb.\n",
        "- **Local Modeller:** Deepseek, Google (Gemma), Kumru vb.\n",
        "- **Model Büyüklüğü:** 2B, 7B, 40B \n",
        "- **Multimodal:** Text, image, audio, video vb.\n",
        " \n",
        "### 2.2. Prompt (İstek/Talimat)\n",
        "\n",
        "Prompt, modele ne yapmasını istediğinizi söyleyen metindir.\n",
        "\n",
        "- **Prompt Anatomisi**:\n",
        "```\n",
        "[System Instruction] + [Context] + [Task] + [Format] + [Examples]\n",
        "```\n",
        "\n",
        "- **Prompt Kalitesi = Output Kalitesi**\n",
        "\n",
        "### 2.3. Model Parametreleri\n",
        "\n",
        "| Parametre | Açıklama | Tipik Aralık |\n",
        "|-----------|----------|-------------|\n",
        "| **temperature** | Örnekleme rastgeleliği. Düşük = tutarlı, yüksek = yaratıcı | 0.0 - 2.0 |\n",
        "| **max_output_tokens** | Tek yanıttaki azami token sayısı | 1 - 65536 |\n",
        "| **top_p** | Olasılık kütlesinden çekirdek örnekleme eşiği | 0.0 - 1.0 |\n",
        "| **top_k** | En olası k aday token arasından seçim | 1 - 40+ |\n",
        "| **thinking_budget** | Düşünme token bütçesi (0=kapalı, -1=otomatik) | 0 - 24576 |\n",
        "\n",
        "\n",
        "### 2.4. Ek Enstrümanlar\n",
        "\n",
        "- **Safety Settings**: Zararlı içerik filtreleme\n",
        "- **Function Calling**: External tool'lara erişim\n",
        "- **Response Schema**: Structured output (JSON)\n",
        "- **Thinking Config**: Modelin düşünme davranışını kontrol etme\n",
        "- **Code Execution**: Modelin Python kodu çalıştırabilmesi\n",
        "- **vb.**\n",
        "\n",
        "## 3. Gemini Modelleri - [Gemini Docs](https://ai.google.dev/gemini-api/docs?hl=tr)\n",
        "\n",
        "| Özellik | Gemini 3 Flash Preview |\n",
        "|---------|----------------------|\n",
        "| Context Window | 1M input token |\n",
        "| Max Output | 65.5K token |\n",
        "| Thinking | Varsayılan açık |\n",
        "| Multimodal | Text, image, audio, video |\n",
        "\n",
        "\n",
        "\n",
        "## 4. Gemini API Key Alma ve Kurulum\n",
        "\n",
        "### 4.1. API Key Alma\n",
        "\n",
        "1. **Google AI Studio'ya gidin**: [https://aistudio.google.com/](https://aistudio.google.com/)\n",
        "2. Google hesabınızla giriş yapın\n",
        "3. Sol menüden **\"Get API Key\"** seçeneğine tıklayın\n",
        "4. **\"Create API Key\"** butonuna basın\n",
        "5. Yeni bir API key oluşturun veya mevcut bir projeye ekleyin\n",
        "6. API key'inizi kopyalayın ve güvenli bir yerde saklayın\n",
        "\n",
        "**Güvenlik Uyarısı**: API key'inizi asla public repository'lere commit etmeyin! [(.gitignore)](https://github.com/github/gitignore/blob/main/Python.gitignore)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79d4004a-0f6e-4bdf-b22c-0ed08bd2f0b1",
      "metadata": {},
      "source": [
        "### 4.2. Python SDK Kurulumu\n",
        "\n",
        "```bash\n",
        "# Google Gen AI SDK'sını yükleyin\n",
        "pip install -q -U google-genai\n",
        "\n",
        "# Alternatif: requirements.txt dosyasına ekleyin\n",
        "\"google-genai>=1.0.0\" \n",
        "pip install -r requirements.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b54bf338-46a4-44b3-91f2-e3ac63c63b8d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# kurulum\n",
        "#pip install -q -U google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0899c839-4059-4119-9e93-27ffe5f09c55",
      "metadata": {},
      "source": [
        "### 4.3. İlk Yapılandırma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45cdf0ef-0f76-4551-9977-2c835bb6527e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "api_key = os.getenv('GEMINI_API_KEY')\n",
        "client = genai.Client(api_key=api_key)\n",
        "\n",
        "#MODEL = 'gemini-3-flash-preview'\n",
        "MODEL = 'gemini-2.5-flash'\n",
        "\n",
        "# Thinking off config - disable model's internal reasoning\n",
        "THINK_OFF = types.ThinkingConfig(thinking_budget=0)\n",
        "\n",
        "#Extract only text parts from response, skipping thought_signature\n",
        "def get_text(response):\n",
        "    return \"\".join(\n",
        "        part.text for part in response.candidates[0].content.parts\n",
        "        if part.text and not part.thought\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42b62692-c0cd-4ec2-9f2b-44ec9987fb45",
      "metadata": {},
      "outputs": [],
      "source": [
        "# check model info\n",
        "model_info = client.models.get(model=MODEL)\n",
        "model_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94a74dad-3fce-40d3-9f40-6c5a4b2ee62e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# thinking ON - model responds with internal reasoning (default behavior)\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents='Selamlar nasılsın, şuan bir eğitimdeyiz herkese selam söyle.',\n",
        "    config=types.GenerateContentConfig(\n",
        "        thinking_config=types.ThinkingConfig(include_thoughts=True)\n",
        "    )\n",
        ")\n",
        "# show thinking parts\n",
        "for part in response.candidates[0].content.parts:\n",
        "    if part.thought:\n",
        "        print(\"[THINKING]\", part.text, \"...\")\n",
        "    else:\n",
        "        print(\"[RESPONSE]\", part.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "002d1524",
      "metadata": {},
      "outputs": [],
      "source": [
        "# thinking OFF - model responds directly without reasoning\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents='Selamlar nasılsın, şuan bir eğitimdeyiz herkese selam söyle.',\n",
        "    config=types.GenerateContentConfig(thinking_config=THINK_OFF)\n",
        ")\n",
        "print(get_text(response))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fec331a-db03-4b6b-bc6a-a6ab32fc84b6",
      "metadata": {},
      "source": [
        "## Parameters\n",
        "\n",
        "### Temperature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e85a4a36-2597-4abb-845c-d853cbd53613",
      "metadata": {},
      "outputs": [],
      "source": [
        "# default temperature - varied outputs each time\n",
        "outputs = []\n",
        "prompt = \"Türkiye Yapay Zeka Topluluğu hakkında sadece 1 cümlelik bilgi ver.\"\n",
        "config = types.GenerateContentConfig(temperature=2, thinking_config=THINK_OFF)\n",
        "for i in range(5):\n",
        "    response = client.models.generate_content(model=MODEL, contents=prompt, config=config)\n",
        "    outputs.append(get_text(response))\n",
        "for index, sentence in enumerate(outputs, start=1):\n",
        "    print(f\"{index}. {sentence}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55044134-365c-4607-96bd-cfa85566872d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# temperature=0 - deterministic, same output each time\n",
        "new_outputs = []\n",
        "low_temp_config = types.GenerateContentConfig(temperature=0, thinking_config=THINK_OFF)\n",
        "for i in range(5):\n",
        "    response = client.models.generate_content(\n",
        "        model=MODEL, contents=prompt, config=low_temp_config\n",
        "    )\n",
        "    new_outputs.append(get_text(response))\n",
        "for index, sentence in enumerate(new_outputs, start=1):\n",
        "    print(f\"{index}. {sentence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "734178a9-de2d-44c5-9656-a972310785d7",
      "metadata": {},
      "source": [
        "## Max Output Length\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b3ac4e7-81b5-4348-80fc-4e968747a13d",
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# no max_output_tokens limit - model decides output length\n",
        "prompt = \"Türkiye Yapay Zeka Topluluğu hakkında bilgi verir misiniz?\"\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL, contents=prompt,\n",
        "    config=types.GenerateContentConfig(thinking_config=THINK_OFF)\n",
        ")\n",
        "print(get_text(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85750e51-df5a-43b9-8ebb-92d6d253b873",
      "metadata": {},
      "outputs": [],
      "source": [
        "# max_output_tokens=200 - limits response length\n",
        "prompt = \"Türkiye Yapay zeka Topluluğu hakkında bilgi verir misiniz?\"\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(max_output_tokens=200, thinking_config=THINK_OFF)\n",
        ")\n",
        "print(get_text(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25a7753b-d759-4424-8f05-99d344a29420",
      "metadata": {},
      "source": [
        "## Token Count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf68c3dd-f854-475c-b9e3-381de31d7aeb",
      "metadata": {},
      "outputs": [],
      "source": [
        "poem_prompt = \"Bilgisayarlar hakkında az bilinen 5 bilgi ver.\"\n",
        "token_config = types.GenerateContentConfig(temperature=0.5, thinking_config=THINK_OFF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a89bbabd-7dcb-4c58-94ba-72d499b4385d",
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL, contents=poem_prompt, config=token_config\n",
        ")\n",
        "print(get_text(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd867277-684f-4f21-9134-a19794822bd1",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_token_count = client.models.count_tokens(model=MODEL, contents=poem_prompt)\n",
        "output_token_count = client.models.count_tokens(model=MODEL, contents=get_text(response))\n",
        "print(f'Tokens in prompt: {prompt_token_count.total_tokens}')\n",
        "print(f'Estimated tokens in output: {output_token_count.total_tokens}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd0330f8-1452-45c0-a1bc-2ff2094ca990",
      "metadata": {},
      "source": [
        "## Prompt Teknikleri\n",
        "\n",
        "### Açık ve Net Talimatlar\n",
        "\n",
        "Prompt kalitesi doğrudan çıktı kalitesini belirler. Aşağıda kötü ve iyi prompt örnekleri:\n",
        "\n",
        "**Kötü Prompt:** Belirsiz, model ne istediğinizi tahmin etmek zorunda\n",
        "```python\n",
        "prompt = \"Python hakkında bir şeyler söyle.\"\n",
        "```\n",
        "\n",
        "**İyi Prompt:** Spesifik görev, format ve kapsam belirli\n",
        "```python\n",
        "prompt = \"\"\"\n",
        "Python programlama dilinin aşağıdaki özelliklerini açıkla:\n",
        "1. Liste comprehension nedir ve nasıl kullanılır?\n",
        "2. Decorator'lar ne işe yarar?\n",
        "3. Generator fonksiyonları neden kullanılır?\n",
        "\n",
        "Her madde için bir kod örneği ver.\n",
        "\"\"\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a26ae4d5-c594-426b-89a8-7cad0945fbaa",
      "metadata": {},
      "source": [
        "### System Prompt\n",
        "\n",
        "System instruction, modele bir kimlik ve davranış kuralları verir. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a56ed016-b1c7-4c88-8e29-a473601b8695",
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "response_with_sys = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=\"Yapay zeka nedir?\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        thinking_config=THINK_OFF,\n",
        "        system_instruction=\"\"\"Sen Türkiye Yapay Zeka Topluluğunun akil küpü yapay zeka asistanisin. \n",
        "        Kullanıcılara sadece Türkiye yapay zeka dışında bir bilgi verme, sadece bu bilgilendirmeyi yap.\"\"\"\n",
        "    )\n",
        ")\n",
        "print(\"=== System Instruction VAR ===\")\n",
        "print(get_text(response_with_sys))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86481a15-50a3-44ba-8b86-05e982a793bd",
      "metadata": {},
      "source": [
        "### Role-Based Prompting (Rol Tanımlama)\n",
        "\n",
        "Aynı soruyu farklı rollerle sorarak çıktının nasıl değiştiğini görelim."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "623f8378",
      "metadata": {},
      "outputs": [],
      "source": [
        "question = \"Bir e-ticaret sitesinde ürün arama performansı düşük. Ne yapmalıyız?\"\n",
        "\n",
        "roles = {\n",
        "    \"Junior Developer\": \"Sen 1 yıllık deneyime sahip junior bir yazılımcısın. Kısa ve basit öneriler ver.\",\n",
        "    \"Senior Architect\": \"Sen 15 yıllık deneyimli bir yazılım mimarısın. Sistem tasarımı perspektifinden yanıt ver.\",\n",
        "    \"Product Manager\": \"Sen bir ürün yöneticisisin. Kullanıcı deneyimi ve iş metrikleri perspektifinden yanıt ver.\"\n",
        "}\n",
        "\n",
        "role_list = list(roles.items())\n",
        "\n",
        "def get_role_response(role_index):\n",
        "    role_name, instruction = role_list[role_index]\n",
        "    response = client.models.generate_content(\n",
        "        model=MODEL, contents=question,\n",
        "        config=types.GenerateContentConfig(\n",
        "            thinking_config=THINK_OFF,\n",
        "            system_instruction=instruction,\n",
        "            max_output_tokens=500\n",
        "        )\n",
        "    )\n",
        "    print(f\"--- {role_name} ---\")\n",
        "    print(get_text(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9714b26",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Junior Developer\n",
        "get_role_response(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b89e53d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Senior Architect\n",
        "get_role_response(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5822a4b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Product Manager\n",
        "get_role_response(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5c09acb",
      "metadata": {},
      "source": [
        "### Structured Output (JSON)\n",
        "\n",
        "Modelden yapılandırılmış veri çıkarma. Aşağıda bir iş ilanından bilgileri JSON formatında çıkarıyoruz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3464ceea",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "job_posting = \"\"\"\n",
        "Pozisyon: Senior Data Scientist\n",
        "Şirket: TrAI Yazılım A.Ş.\n",
        "Lokasyon: İstanbul (Hibrit - Haftada 2 gün ofis)\n",
        "Maaş Aralığı: 150.000 - 200.000 TL\n",
        "Gereksinimler:\n",
        "- Python, SQL ve Spark deneyimi (en az 4 yıl)\n",
        "- Makine öğrenimi model geliştirme tecrübesi\n",
        "- İyi derecede İngilizce\n",
        "- Üniversite mezunu (Bilgisayar Mühendisliği, İstatistik veya ilgili alan)\n",
        "Arti Nitelikler: MLOps, Docker, AWS deneyimi\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"Aşağıdaki iş ilanından bilgileri çıkar ve SADECE JSON formatında döndür:\n",
        "\n",
        "{job_posting}\n",
        "\n",
        "JSON şeması:\n",
        "{{\n",
        "  \"position\": \"string\",\n",
        "  \"company\": \"string\",\n",
        "  \"location\": \"string\",\n",
        "  \"work_model\": \"string\",\n",
        "  \"salary_min\": number,\n",
        "  \"salary_max\": number,\n",
        "  \"min_experience_years\": number,\n",
        "  \"required_skills\": [\"string\"],\n",
        "  \"nice_to_have\": [\"string\"],\n",
        "  \"education\": \"string\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL, contents=prompt,\n",
        "    config=types.GenerateContentConfig(thinking_config=THINK_OFF)\n",
        ")\n",
        "result = get_text(response)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "111b1696-be58-46dd-8bdc-e44a7b7aa740",
      "metadata": {},
      "source": [
        "## Zero-Shot Prompting\n",
        "\n",
        "Hiç örnek vermeden, sadece görev tanımıyla model yönlendirme. Aşağıda bir müşteri destek mesajını otomatik kategorilere ayırıyoruz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7ebf46f-56f4-4c15-b2e0-0efee62438fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Zero-shot: Hiç örnek vermeden kategori belirleme\n",
        "tickets = [\n",
        "    \"Siparişim 5 gündür gelmedi, kargo nerede? Çok sinirli oldum artık!\",\n",
        "    \"Ürünlerinize renk filtresi ekleseniz çok iyi olur, aramayı kolaylaştırır.\",\n",
        "    \"Geçen hafta aldığım laptop hakkında garanti süresini öğrenmek istiyorum.\",\n",
        "    \"Harika bir alışveriş deneyimiydi, müşteri hizmetleri çok ilgiliydi, teşekkürler!\"\n",
        "]\n",
        "\n",
        "prompt_template = \"\"\"Aşağıdaki müşteri mesajını analiz et.\n",
        "Kategori: şikayet / öneri / bilgi_talebi / teşekkür\n",
        "Aciliyet: düşük / orta / yüksek\n",
        "Sadece bu iki bilgiyi ver, açıklama ekleme.\n",
        "\n",
        "Mesaj: \"{ticket}\"\n",
        "\"\"\"\n",
        "\n",
        "for ticket in tickets:\n",
        "    response = client.models.generate_content(\n",
        "        model=MODEL,\n",
        "        contents=prompt_template.format(ticket=ticket),\n",
        "        config=types.GenerateContentConfig(thinking_config=THINK_OFF, max_output_tokens=50)\n",
        "    )\n",
        "    print(f\"Mesaj: {ticket[:60]}...\")\n",
        "    print(get_text(response))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c05adf86-77de-4d06-bc80-709a0c3a186f",
      "metadata": {},
      "source": [
        "## Few-Shot Learning (Örneklerle Öğretme)\n",
        "\n",
        "Birkaç örnek vererek modele custom bir format/davranış öğretme. Zero-shot'tan farkı: model görmediği bir format veya kural setini örneklerden öğrenir. Aşağıda yapılandırılmamış e-posta metninden veri çıkarmayı öğretiyoruz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faf30803-ef1c-4e93-b399-d325e8f399bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Few-shot: Örneklerle custom format öğretme\n",
        "prompt = \"\"\"E-posta metninden yapılandırılmış veri çıkar.\n",
        "\n",
        "--- ÖRNEK 1 ---\n",
        "E-posta: \"Merhaba, ben Ayşe Kara. 15 Ocak'ta sipariş ettiğim #ORD-4521 numaralı ürün hasarlı geldi. Faturamı da bulamıyorum. İade yapmak istiyorum. Tel: 0532 111 22 33\"\n",
        "Çıktı:\n",
        "- Müşteri: Ayşe Kara\n",
        "- Sipariş No: #ORD-4521\n",
        "- Sorun: Hasarlı ürün\n",
        "- Talep: İade\n",
        "- İletişim: 0532 111 22 33\n",
        "\n",
        "--- ÖRNEK 2 ---\n",
        "E-posta: \"Mehmet Yıldız yazıyorum. Dün aldığım monitörün (#ORD-7890) ölü pikseli var, değişim talep ediyorum. Mail: mehmet@email.com\"\n",
        "Çıktı:\n",
        "- Müşteri: Mehmet Yıldız\n",
        "- Sipariş No: #ORD-7890\n",
        "- Sorun: Ölü piksel\n",
        "- Talep: Değişim\n",
        "- İletişim: mehmet@email.com\n",
        "\n",
        "--- ŞİMDİ SEN ÇÖZ ---\n",
        "E-posta: \"Selam, Zeynep Demir. Geçen hafta sipariş verdiğim kulaklık (#ORD-3156) kutusunda şarj kablosu eksik, tamamlayabilir misiniz? Bana 0555 987 65 43 ten ulaşabilirsiniz.\"\n",
        "Çıktı:\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL, contents=prompt,\n",
        "    config=types.GenerateContentConfig(thinking_config=THINK_OFF, max_output_tokens=150)\n",
        ")\n",
        "print(get_text(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ac09388-1d50-431e-8ce3-8094d06379db",
      "metadata": {},
      "source": [
        "## Chain-of-Thought (Düşünce Zinciri)\n",
        "\n",
        "Modelden adım adım muhakeme yapmasını isteyerek daha doğru sonuçlar elde etme. Basit hesaplamalar yerine, birden fazla kriteri tartmayı gerektiren karmaşık bir teknik karar problemi verelim."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9823515-2570-4abb-afb1-3d58e3238853",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Adım adım düşünerek aşağıdaki teknik kararı analiz et:\n",
        "\n",
        "SENARYO:\n",
        "Bir startup, günlük 50.000 kullanıcının etkileşimde bulunduğu bir sosyal medya uygulaması geliştiriyor. \n",
        "Kullanıcılar post paylaşıyor, yorum yapıyor ve birbirini takip ediyor.\n",
        "Gelecek 6 ayda 500.000 kullanıcıya ölçeklenmeyi planlıyorlar.\n",
        "\n",
        "SORU: Veritabanı olarak PostgreSQL mu yoksa MongoDB mi tercih etmeliler?\n",
        "\n",
        "ANALİZ FORMATI:\n",
        "1. Veri yapısını analiz et (ilişkisel mi, döküman tabanlı mı?)\n",
        "2. Her seçenek için avantaj ve dezavantajları listele\n",
        "3. Ölçeklenme gereksinimlerini değerlendir\n",
        "4. Nihai önerini gerekçesiyle sun\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL, contents=prompt,\n",
        "    config=types.GenerateContentConfig(thinking_config=THINK_OFF, max_output_tokens=800)\n",
        ")\n",
        "print(get_text(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f85ddf0a-f9bf-4252-85e8-6bcf8b6efecb",
      "metadata": {},
      "source": [
        "## Bağlamı koruyarak chat in devam etmesi\n",
        "\n",
        "Gemini SDK `chats.create()` ile başlattığınız oturumda **conversation memory** otomatik çalışır: her `send_message` çağrısında önceki tüm mesajlar modele gönderilir, böylece model bağlamı korur. Aşağıda önce bağlam olmadan ne olacağını, sonra bağlamlı sohbeti ve geçmişi nasıl inceleyeceğinizi görüyorsunuz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff1c736c-a8b5-4a45-be8b-2e83c892963d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chat oturumu başlat\n",
        "chat = client.chats.create(\n",
        "    model=MODEL,\n",
        "    config=types.GenerateContentConfig(thinking_config=THINK_OFF, max_output_tokens=800)\n",
        ")\n",
        "\n",
        "# İlk mesaj\n",
        "response1 = chat.send_message(\"Rust öğrenmek istiyorum nereden başlayabilirim?\")\n",
        "print(\"Bot:\", get_text(response1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8702b095-c117-43d3-b2ac-da23a0822ea1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# İkinci mesaj (bağlam korunur)\n",
        "response2 = chat.send_message(\"Peki, hangi IDE'yi önerirsin?\")\n",
        "print(\"Bot:\", get_text(response2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1309257-364a-4df8-b7bd-a0d76dd53408",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Üçüncü mesaj\n",
        "response3 = chat.send_message(\"Bu programlama dilinin ana olayı nedir?\")\n",
        "print(\"Bot:\", get_text(response3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6527b537-727c-47d9-8b2d-de086dbb52a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sohbet geçmişini görüntüle\n",
        "print(\"\\n--- Sohbet Geçmişi ---\")\n",
        "for message in chat.get_history():\n",
        "    print(f\"{message.role}: {message.parts[0].text}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f4b3811-c7cc-40ed-aa2e-4d869e5d401d",
      "metadata": {},
      "source": [
        "## Hepsini bir araya getirelim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d590bc7d-9d68-44b7-9b79-86ba04f88be6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import gradio as gr\n",
        "\n",
        "load_dotenv()\n",
        "gr_client = genai.Client(api_key=os.getenv('GEMINI_API_KEY'))\n",
        "\n",
        "\n",
        "def get_text(response):\n",
        "    return \"\".join(\n",
        "        part.text for part in response.candidates[0].content.parts\n",
        "        if part.text and not part.thought\n",
        "    )\n",
        "\n",
        "CHAT_CONFIG = types.GenerateContentConfig(\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    top_k=40,\n",
        "    max_output_tokens=2048,\n",
        "    thinking_config=types.ThinkingConfig(thinking_budget=0),\n",
        "    system_instruction=\"\"\"Sen KrediPusula projesinin kredi risk danışmanısın. KrediPusula, kullanıcıların kredi uygunluğunu analiz eden, kişiselleştirilmiş kredi önerileri sunan akıllı kredi danışmanlık platformudur.\n",
        "\n",
        "Görevin:\n",
        "- Kredi riski, kredi skoru ve kredi uygunluğu hakkında bilgi vermek\n",
        "- Kullanıcıları gelir, yaş, meslek, tasarruf durumu gibi faktörlerin kredi başvurusuna etkisi konusunda yönlendirmek\n",
        "- Kredi hesaplama, faiz oranları ve taksit seçenekleri hakkında genel bilgi sunmak\n",
        "- Başvuru süreci, gerekli belgeler ve platformun nasıl kullanılacağı konusunda yardımcı olmak\n",
        "\n",
        "Kurallar:\n",
        "- Yalnızca kredi, risk ve KrediPusula platformu ile ilgili sorulara yanıt ver\n",
        "- Kesin onay/red kararı verme; nihai karar model ve banka süreçlerine aittir\n",
        "- Türkçe yanıt ver, kısa ve anlaşılır ol\n",
        "- Bilmediğin konularda tahmin yürütme, platformdaki başvuru formunu öner\"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "def chat_function(message, history):\n",
        "    if not message or message.strip() == \"\":\n",
        "        return \"Lütfen bir mesaj yazın.\"\n",
        "\n",
        "    chat_history = []\n",
        "    for human, assistant in history:\n",
        "        if human and assistant:\n",
        "            chat_history.append(\n",
        "                types.Content(role=\"user\", parts=[types.Part.from_text(text=human)])\n",
        "            )\n",
        "            chat_history.append(\n",
        "                types.Content(role=\"model\", parts=[types.Part.from_text(text=assistant)])\n",
        "            )\n",
        "\n",
        "    chat = gr_client.chats.create(\n",
        "        model='gemini-3-flash-preview', config=CHAT_CONFIG, history=chat_history\n",
        "    )\n",
        "    response = chat.send_message(message.strip())\n",
        "    return get_text(response)\n",
        "\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat_function,\n",
        "    title=\"KrediPusula - Kredi Risk Danışmanı\",\n",
        "    description=\"Kredi uygunluğu, risk skoru ve kredi başvurusu hakkında sorularınızı sorun\",\n",
        "    examples=[\n",
        "        \"Kredi risk skoru nedir, nasıl hesaplanır?\",\n",
        "        \"Gelirim düşük, kredi alabilir miyim?\",\n",
        "        \"Kredi başvurusu için hangi belgeler gerekli?\",\n",
        "        \"Tasarruf hesabım yok, bu kredi onayımı etkiler mi?\"\n",
        "    ],\n",
        "    theme=\"soft\"\n",
        ")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(share=True)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
